{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will create a custom Named Entity Recognition model for this task, Since it will be suitable to extract the required information (who acquires the company, who is the company being acquired, what is the sell price). We need labeled data for this task. Since I couldn't find any datasets on the internet, I need to create my own. To do that we first need to scrape the news websites. We can use tools like BeautifulSoup or Selenium to extract news from google. We can only query \"mergers and acquisitions\", the \"in Germany\" part is not relevant for training data. Also, it will give us more results this way. I also searched in english for the same reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get at most 1000 articles with one query (google puts a limit). Also there isn't enough articles with one query to \"mergers and acquisitions\". To get more, we need to search by date and accumulate the news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 31 articles for date range 2023-07-13 to 2023-07-13\n",
      "Fetched 2 articles for date range 2023-07-15 to 2023-07-15\n",
      "Fetched 21 articles for date range 2023-07-14 to 2023-07-14\n",
      "Fetched 26 articles for date range 2023-07-12 to 2023-07-12\n",
      "Fetched 2 articles for date range 2023-07-16 to 2023-07-16\n",
      "Fetched 40 articles for date range 2023-07-19 to 2023-07-19\n",
      "Fetched 23 articles for date range 2023-07-17 to 2023-07-17\n",
      "Fetched 29 articles for date range 2023-07-18 to 2023-07-18\n",
      "Fetched 17 articles for date range 2023-07-21 to 2023-07-21\n",
      "Fetched 3 articles for date range 2023-07-23 to 2023-07-23\n",
      "Fetched 42 articles for date range 2023-07-20 to 2023-07-20\n",
      "Fetched 19 articles for date range 2023-07-24 to 2023-07-24\n",
      "Fetched 1 articles for date range 2023-07-22 to 2023-07-22\n",
      "Fetched 27 articles for date range 2023-07-26 to 2023-07-26\n",
      "Fetched 23 articles for date range 2023-07-27 to 2023-07-27\n",
      "Fetched 1 articles for date range 2023-07-29 to 2023-07-29\n",
      "Fetched 20 articles for date range 2023-07-25 to 2023-07-25\n",
      "Fetched 17 articles for date range 2023-07-28 to 2023-07-28\n",
      "Fetched 2 articles for date range 2023-07-30 to 2023-07-30\n",
      "Fetched 38 articles for date range 2023-08-02 to 2023-08-02\n",
      "Fetched 17 articles for date range 2023-07-31 to 2023-07-31\n",
      "Fetched 28 articles for date range 2023-08-03 to 2023-08-03\n",
      "Fetched 29 articles for date range 2023-08-01 to 2023-08-01\n",
      "Fetched 4 articles for date range 2023-08-06 to 2023-08-06\n",
      "Fetched 17 articles for date range 2023-08-04 to 2023-08-04\n",
      "Fetched 29 articles for date range 2023-08-07 to 2023-08-07\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import concurrent.futures\n",
    "\n",
    "def fetch_news_articles(query, start_date, end_date):\n",
    "    base_url = \"https://www.google.com/search\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"tbm\": \"nws\",\n",
    "        \"tbs\": f\"cdr:1,cd_min:{start_date.strftime('%m/%d/%Y')},cd_max:{end_date.strftime('%m/%d/%Y')}\",\n",
    "        \"num\": 100 #to get the most number of articles in one page (google allows at most 100)\n",
    "    }\n",
    "    \n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles = []\n",
    "        for item in soup.select(\"div.SoaBEf\"):\n",
    "            title_elem = item.select_one(\"div.MBeuO\")\n",
    "            link_elem = item.select_one(\"a\")\n",
    "            source_elem = item.select_one(\"div.UPmit\")\n",
    "            \n",
    "            if title_elem and link_elem:\n",
    "                article = {\n",
    "                    \"title\": title_elem.text.strip(),\n",
    "                    \"link\": link_elem[\"href\"],\n",
    "                    \"source\": source_elem.text.strip() if source_elem else \"N/A\"\n",
    "                }\n",
    "                articles.append(article)\n",
    "        \n",
    "        print(f\"Fetched {len(articles)} articles for date range {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "        return articles\n",
    "    else:\n",
    "        print(f\"Failed to fetch articles. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def fetch_articles_with_date_ranges(query):\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=365)\n",
    "    #search every day for the last year\n",
    "    date_ranges = [(start_date + timedelta(days=i), start_date + timedelta(days=i)) for i in range((end_date - start_date).days + 1)]\n",
    "\n",
    "    all_articles = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        future_to_date_range = {executor.submit(fetch_news_articles, query, start_date, end_date): (start_date, end_date) \n",
    "                                for start_date, end_date in date_ranges}\n",
    "        for future in concurrent.futures.as_completed(future_to_date_range):\n",
    "            all_articles.extend(future.result())\n",
    "    \n",
    "    return all_articles\n",
    "\n",
    "# Query for mergers and acquisitions in Germany\n",
    "query = \"mergers and acquisitions\"\n",
    "\n",
    "# Fetch news articles with date ranges\n",
    "articles = fetch_articles_with_date_ranges(query)\n",
    "\n",
    "# Write fetched articles to a CSV file\n",
    "csv_file = \"articles.csv\"\n",
    "csv_columns = [\"title\", \"link\", \"source\"]\n",
    "\n",
    "try:\n",
    "    with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "        writer.writeheader()\n",
    "        for article in articles:\n",
    "            writer.writerow(article)\n",
    "    print(f\"\\nSuccessfully wrote {len(articles)} articles to '{csv_file}'.\")\n",
    "except IOError:\n",
    "    print(f\"\\nError: Could not write to '{csv_file}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have the articles, we can proceed by extracting the main body from each URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml_html_clean\n",
      "  Downloading lxml_html_clean-0.1.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: lxml in c:\\users\\umutc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lxml_html_clean) (5.2.2)\n",
      "Downloading lxml_html_clean-0.1.1-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: lxml_html_clean\n",
      "Successfully installed lxml_html_clean-0.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml_html_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted content from https://news.google.com/rss/articles/CBMiemh0dHBzOi8vd3dkLmNvbS9idXNpbmVzcy1uZXdzL21lcmdlcnMtYWNxdWlzaXRpb25zL3NvZGFsaXMtZ3JvdXAtYWNxdWlyZXMtbWFqb3JpdHktc3Rha2UtZ2VybWFuLWJlYXV0eS1hcnRkZWNvLTEyMzYzNzM3Mjkv0gEA?oc=5\n",
      "Extracted content from https://news.google.com/rss/articles/CBMicWh0dHBzOi8vd3d3Lm1kbS5jb20vbmV3cy9vcGVyYXRpb25zL21lcmdlcnMtYWNxdWlzaXRpb25zL2ZvcnRpdmUtdG8tYWNxdWlyZS1nZXJtYW55cy1lbGVrdHJvLWF1dG9tYXRpay1mb3ItMS00NWIv0gEA?oc=5\n",
      "Extracted content from https://news.google.com/rss/articles/CBMikQFodHRwczovL3d3dy5idXNpbmVzcy1zdGFuZGFyZC5jb20vdGVjaG5vbG9neS90ZWNoLW5ld3Mvd2hhdC1tYWtlcy1lbmdpbmVlcmluZy1yLWQtc3BhY2UtYS10YXJnZXQtZm9yLW1lcmdlcnMtYW5kLWFjcXVpc2l0aW9ucy0xMjQwNDI0MDA4MzBfMS5odG1s0gGVAWh0dHBzOi8vd3d3LmJ1c2luZXNzLXN0YW5kYXJkLmNvbS9hbXAvdGVjaG5vbG9neS90ZWNoLW5ld3Mvd2hhdC1tYWtlcy1lbmdpbmVlcmluZy1yLWQtc3BhY2UtYS10YXJnZXQtZm9yLW1lcmdlcnMtYW5kLWFjcXVpc2l0aW9ucy0xMjQwNDI0MDA4MzBfMS5odG1s?oc=5\n",
      "Extracted content from https://news.google.com/rss/articles/CBMia2h0dHBzOi8va3BtZy5jb20veHgvZW4vaG9tZS9pbnNpZ2h0cy8yMDIxLzA0L2dlcm1hbnktdGF4YXRpb24tb2YtY3Jvc3MtYm9yZGVyLW1lcmdlcnMtYW5kLWFjcXVpc2l0aW9ucy5odG1s0gEA?oc=5\n",
      "Failed to extract using newspaper3k: Article `download()` failed with 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/markets/europe/german-ma-outlook-clouded-by-growing-economic-challenges-2024-01-12/ on URL https://news.google.com/rss/articles/CBMia2h0dHBzOi8vd3d3LnJldXRlcnMuY29tL21hcmtldHMvZXVyb3BlL2dlcm1hbi1tYS1vdXRsb29rLWNsb3VkZWQtYnktZ3Jvd2luZy1lY29ub21pYy1jaGFsbGVuZ2VzLTIwMjQtMDEtMTIv0gEA?oc=5\n",
      "Extracted content from https://news.google.com/rss/articles/CBMia2h0dHBzOi8vd3d3LnJldXRlcnMuY29tL21hcmtldHMvZXVyb3BlL2dlcm1hbi1tYS1vdXRsb29rLWNsb3VkZWQtYnktZ3Jvd2luZy1lY29ub21pYy1jaGFsbGVuZ2VzLTIwMjQtMDEtMTIv0gEA?oc=5\n",
      "Extracted content from https://news.google.com/rss/articles/CBMia2h0dHBzOi8vaG9zcGl0YWxpdHktb24uY29tL2VuL21lcmdlcnMtYW5kLWFjcXVpc2l0aW9ucy90d2VudHktdHdvLXJlYWwtZXN0YXRlLWFjcXVpcmVzLWNlbnRlci1wYXJjcy1nZXJtYW550gEA?oc=5\n",
      "Extracted content from https://news.google.com/rss/articles/CBMiP2h0dHBzOi8vd3d3LmZ0LmNvbS9jb250ZW50LzVhZjBkOWVhLTc3MWEtNGRlMC1hMWIwLTk1N2JkYzlmZDY3MtIBAA?oc=5\n",
      "Extracted content from https://news.google.com/rss/articles/CBMiVGh0dHBzOi8vd3d3LmlmbHIuY29tL2FydGljbGUvMmJkbjltaWkzc3E1cnFlc3J0MzQwL3Nwb25zb3JlZC9tLWEtcmVwb3J0LTIwMjMtZ2VybWFuedIBAA?oc=5\n",
      "Extracted content from https://news.google.com/rss/articles/CBMiU2h0dHBzOi8vY2VuLmFjcy5vcmcvYnVzaW5lc3MvbWVyZ2Vycy0mLWFjcXVpc2l0aW9ucy9JQ0lHLWJ1eXMtYWdhaW4tR2VybWFueS8xMDEvaTIw0gEA?oc=5\n",
      "Extracted content from https://news.google.com/rss/articles/CBMiT2h0dHBzOi8vd3d3LnBvbGl0aWNvLmV1L2FydGljbGUvZ2VybWFueS1mcmFuY2UtbWVnYS1kZWFscy1jb21wZXRpdGlvbi1vdmVyaGF1bC_SAQA?oc=5\n",
      "Extracted content from https://news.google.com/rss/articles/CBMiZ2h0dHBzOi8vc2tpZnQuY29tLzIwMjMvMDQvMjUvcHJlbWllci1pbm4tdG8tZXhwYW5kLWJ1ZGdldC1ob3RlbC1lbXBpcmUtaW4tZ2VybWFueS10aHJvdWdoLWFjcXVpc2l0aW9ucy_SAWtodHRwczovL3NraWZ0LmNvbS8yMDIzLzA0LzI1L3ByZW1pZXItaW5uLXRvLWV4cGFuZC1idWRnZXQtaG90ZWwtZW1waXJlLWluLWdlcm1hbnktdGhyb3VnaC1hY3F1aXNpdGlvbnMvYW1wLw?oc=5\n",
      "Successfully wrote articles with content to 'articles_with_content.csv'.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "\n",
    "# Function to extract article content using newspaper3k\n",
    "def extract_content_newspaper(url):\n",
    "    article = Article(url)\n",
    "    try:\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.text\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract using newspaper3k: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract article content using BeautifulSoup as fallback\n",
    "def extract_content_bs(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        return ' '.join([para.get_text() for para in paragraphs])\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract using BeautifulSoup: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract content from a URL\n",
    "def extract_content(url):\n",
    "    content = extract_content_newspaper(url)\n",
    "    if not content:\n",
    "        content = extract_content_bs(url)\n",
    "    return content\n",
    "\n",
    "# Read the URLs from the CSV file\n",
    "csv_file = \"articles.csv\"\n",
    "articles = []\n",
    "\n",
    "try:\n",
    "    with open(csv_file, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            articles.append(row)\n",
    "except IOError:\n",
    "    print(f\"Error: Could not read from '{csv_file}'.\")\n",
    "\n",
    "# Extract content for each article\n",
    "for article in articles:\n",
    "    url = article['link']\n",
    "    content = extract_content(url)\n",
    "    article['content'] = content\n",
    "    print(f\"Extracted content from {url}\")\n",
    "\n",
    "# Write the updated articles with content to a new CSV file\n",
    "output_csv_file = \"articles_with_content.csv\"\n",
    "csv_columns = [\"title\", \"link\", \"content\"]\n",
    "\n",
    "try:\n",
    "    with open(output_csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "        writer.writeheader()\n",
    "        for article in articles:\n",
    "            writer.writerow(article)\n",
    "    print(f\"Successfully wrote articles with content to '{output_csv_file}'.\")\n",
    "except IOError:\n",
    "    print(f\"Error: Could not write to '{output_csv_file}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this code works for a small number of requests. But after doing more requests we get rate limits from google. Thus I needed to use proxies to bypass the limits. But none of them worked, so I gave up trying this approach, and used SerpApi instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
