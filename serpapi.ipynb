{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use SerpAPi's google news search. I have 100 requests in my Free account. I can get at most 100 article per request. So, in total we can have at most 10000 training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-search-results\n",
      "  Using cached google_search_results-2.4.2-py3-none-any.whl\n",
      "Requirement already satisfied: requests in c:\\users\\umutc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-search-results) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\umutc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->google-search-results) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\umutc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->google-search-results) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\umutc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->google-search-results) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\umutc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->google-search-results) (2024.6.2)\n",
      "Installing collected packages: google-search-results\n",
      "Successfully installed google-search-results-2.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install google-search-results \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying one api call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'search_metadata': {'id': '668fe5df47ccf18c40d7ee93', 'status': 'Success', 'json_endpoint': 'https://serpapi.com/searches/bc273cd9b431b314/668fe5df47ccf18c40d7ee93.json', 'created_at': '2024-07-11 14:02:07 UTC', 'processed_at': '2024-07-11 14:02:07 UTC', 'google_url': 'https://www.google.com/search?q=mergers+and+acquisitions&oq=mergers+and+acquisitions&hl=en&gl=us&num=20&tbm=nws&tbs=cdr:1,cd_min:02-03-2024,cd_max:02-04-2024&start=0&sourceid=chrome&ie=UTF-8', 'raw_html_file': 'https://serpapi.com/searches/bc273cd9b431b314/668fe5df47ccf18c40d7ee93.html', 'total_time_taken': 1.68}, 'search_parameters': {'engine': 'google', 'q': 'mergers and acquisitions', 'google_domain': 'google.com', 'hl': 'en', 'gl': 'us', 'start': 0, 'num': '20', 'device': 'desktop', 'tbm': 'nws', 'tbs': 'cdr:1,cd_min:02-03-2024,cd_max:02-04-2024'}, 'search_information': {'query_displayed': 'mergers and acquisitions', 'total_results': 3, 'time_taken_displayed': 0.21, 'news_results_state': 'Results for exact spelling'}, 'news_results': [{'position': 1, 'link': 'https://www.realestatenews.com/2024/02/04/why-brokerage-mergers-are-about-buying-hope', 'title': 'Why brokerage mergers are about buying ‘hope’', 'source': 'RealEstateNews.com', 'date': 'Feb 4, 2024', 'snippet': 'A consultant and former Anywhere exec breaks down the actions investors \\nshould take when looking to acquire or merge with other real estate firms.', 'thumbnail': 'https://serpapi.com/searches/668fe5df47ccf18c40d7ee93/images/48e83a4be96482524812c69a4e4aa4bfe3cab9a747914affa6392258fb7a1145.jpeg'}, {'position': 2, 'link': 'https://www.washingtonpost.com/technology/2024/02/03/trump-social-dwac-investigation/', 'title': 'The wild probe into investors of DWAC, Trump Media’s proposed merger ally', 'source': 'The Washington Post', 'date': 'Feb 3, 2024', 'snippet': 'An investigation into early investors of Digital World Acquisition included \\na government informant, a secret phone scan and an elite \\nanti-money-laundering...', 'thumbnail': 'https://serpapi.com/searches/668fe5df47ccf18c40d7ee93/images/48e83a4be9648252d46ba1b63f8fe0ea55cc3513768a8dfd856673d973ccff60.jpeg'}, {'position': 3, 'link': 'https://fourweekmba.com/google-acquisitions/', 'title': 'Google Acquisitions', 'source': 'FourWeekMBA', 'date': 'Feb 4, 2024', 'snippet': 'With 48 acquisitions in 2010 and 57 in 2011, Google was acquiring companies \\nat a rate of around one per week over this two-year period around a decade \\nago.', 'thumbnail': 'https://serpapi.com/searches/668fe5df47ccf18c40d7ee93/images/48e83a4be96482525ee39f39d61219ac91af8a89a95c06f57e6805b8030695d7.jpeg'}]}\n"
     ]
    }
   ],
   "source": [
    "from serpapi.google_search import GoogleSearch\n",
    "\n",
    "params = {\n",
    "  \"engine\": \"google\",\n",
    "  \"q\": \"mergers and acquisitions\",\n",
    "  \"google_domain\": \"google.com\",\n",
    "  \"gl\": \"us\",\n",
    "  \"hl\": \"en\",\n",
    "  \"tbm\": \"nws\",\n",
    "  \"num\": \"20\",\n",
    "  \"start\": \"0\",\n",
    "  \"tbs\": \"cdr:1,cd_min:02-03-2024,cd_max:02-04-2024\",\n",
    "  \"api_key\": \"mykey\"\n",
    "}\n",
    "\n",
    "search = GoogleSearch(params)\n",
    "results = search.get_dict()\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On experimenting, I saw that you get approximately 100 articles if you request with one week range. Thus we will request 7 days 100 times. we have 52 weeks in a year, so we need to search news up to two years ago. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for week: 07-05-2022 to 07-12-2022\n",
      "Found 100 articles\n",
      "Searching for week: 07-12-2022 to 07-19-2022\n",
      "Found 100 articles\n",
      "Searching for week: 07-19-2022 to 07-26-2022\n",
      "Found 100 articles\n",
      "Searching for week: 07-26-2022 to 08-02-2022\n",
      "Found 100 articles\n",
      "Searching for week: 08-02-2022 to 08-09-2022\n",
      "Found 97 articles\n",
      "Searching for week: 08-09-2022 to 08-16-2022\n",
      "Found 100 articles\n",
      "Searching for week: 08-16-2022 to 08-23-2022\n",
      "Found 94 articles\n",
      "Searching for week: 08-23-2022 to 08-30-2022\n",
      "Found 100 articles\n",
      "Searching for week: 08-30-2022 to 09-06-2022\n",
      "Found 100 articles\n",
      "Searching for week: 09-06-2022 to 09-13-2022\n",
      "Found 100 articles\n",
      "Searching for week: 09-13-2022 to 09-20-2022\n",
      "Found 89 articles\n",
      "Searching for week: 09-20-2022 to 09-27-2022\n",
      "Found 100 articles\n",
      "Searching for week: 09-27-2022 to 10-04-2022\n",
      "Found 100 articles\n",
      "Searching for week: 10-04-2022 to 10-11-2022\n",
      "Found 100 articles\n",
      "Searching for week: 10-11-2022 to 10-18-2022\n",
      "Found 100 articles\n",
      "Searching for week: 10-18-2022 to 10-25-2022\n",
      "Found 100 articles\n",
      "Searching for week: 10-25-2022 to 11-01-2022\n",
      "Found 74 articles\n",
      "Searching for week: 11-01-2022 to 11-08-2022\n",
      "Found 100 articles\n",
      "Searching for week: 11-08-2022 to 11-15-2022\n",
      "Found 100 articles\n",
      "Searching for week: 11-15-2022 to 11-22-2022\n",
      "Found 100 articles\n",
      "Searching for week: 11-22-2022 to 11-29-2022\n",
      "Found 95 articles\n",
      "Searching for week: 11-29-2022 to 12-06-2022\n",
      "Found 100 articles\n",
      "Searching for week: 12-06-2022 to 12-13-2022\n",
      "Found 100 articles\n",
      "Searching for week: 12-13-2022 to 12-20-2022\n",
      "Found 100 articles\n",
      "Searching for week: 12-20-2022 to 12-27-2022\n",
      "Found 84 articles\n",
      "Searching for week: 12-27-2022 to 01-03-2023\n",
      "Found 100 articles\n",
      "Searching for week: 01-03-2023 to 01-10-2023\n",
      "Found 100 articles\n",
      "Searching for week: 01-10-2023 to 01-17-2023\n",
      "Found 99 articles\n",
      "Searching for week: 01-17-2023 to 01-24-2023\n",
      "Found 88 articles\n",
      "Searching for week: 01-24-2023 to 01-31-2023\n",
      "Found 100 articles\n",
      "Searching for week: 01-31-2023 to 02-07-2023\n",
      "Found 100 articles\n",
      "Searching for week: 02-07-2023 to 02-14-2023\n",
      "Found 100 articles\n",
      "Searching for week: 02-14-2023 to 02-21-2023\n",
      "Found 100 articles\n",
      "Searching for week: 02-21-2023 to 02-28-2023\n",
      "Found 100 articles\n",
      "Searching for week: 02-28-2023 to 03-07-2023\n",
      "Found 100 articles\n",
      "Searching for week: 03-07-2023 to 03-14-2023\n",
      "Found 100 articles\n",
      "Searching for week: 03-14-2023 to 03-21-2023\n",
      "Found 100 articles\n",
      "Searching for week: 03-21-2023 to 03-28-2023\n",
      "Found 100 articles\n",
      "Searching for week: 03-28-2023 to 04-04-2023\n",
      "Found 100 articles\n",
      "Searching for week: 04-04-2023 to 04-11-2023\n",
      "Found 100 articles\n",
      "Searching for week: 04-11-2023 to 04-18-2023\n",
      "Found 95 articles\n",
      "Searching for week: 04-18-2023 to 04-25-2023\n",
      "Found 100 articles\n",
      "Searching for week: 04-25-2023 to 05-02-2023\n",
      "Found 100 articles\n",
      "Searching for week: 05-02-2023 to 05-09-2023\n",
      "Found 100 articles\n",
      "Searching for week: 05-09-2023 to 05-16-2023\n",
      "Found 100 articles\n",
      "Searching for week: 05-16-2023 to 05-23-2023\n",
      "Found 95 articles\n",
      "Searching for week: 05-23-2023 to 05-30-2023\n",
      "Found 100 articles\n",
      "Searching for week: 05-30-2023 to 06-06-2023\n",
      "Found 100 articles\n",
      "Searching for week: 06-06-2023 to 06-13-2023\n",
      "Found 100 articles\n",
      "Searching for week: 06-13-2023 to 06-20-2023\n",
      "Found 100 articles\n",
      "Searching for week: 06-20-2023 to 06-27-2023\n",
      "Found 100 articles\n",
      "Searching for week: 06-27-2023 to 07-04-2023\n",
      "Found 66 articles\n",
      "Searching for week: 07-04-2023 to 07-11-2023\n",
      "Found 100 articles\n",
      "Searching for week: 07-11-2023 to 07-18-2023\n",
      "Found 100 articles\n",
      "Searching for week: 07-18-2023 to 07-25-2023\n",
      "Found 100 articles\n",
      "Searching for week: 07-25-2023 to 08-01-2023\n",
      "Found 100 articles\n",
      "Searching for week: 08-01-2023 to 08-08-2023\n",
      "Found 100 articles\n",
      "Searching for week: 08-08-2023 to 08-15-2023\n",
      "Found 87 articles\n",
      "Searching for week: 08-15-2023 to 08-22-2023\n",
      "Found 100 articles\n",
      "Searching for week: 08-22-2023 to 08-29-2023\n",
      "Found 100 articles\n",
      "Searching for week: 08-29-2023 to 09-05-2023\n",
      "Found 100 articles\n",
      "Searching for week: 09-05-2023 to 09-12-2023\n",
      "Found 100 articles\n",
      "Searching for week: 09-12-2023 to 09-19-2023\n",
      "Found 100 articles\n",
      "Searching for week: 09-19-2023 to 09-26-2023\n",
      "Found 85 articles\n",
      "Searching for week: 09-26-2023 to 10-03-2023\n",
      "Found 100 articles\n",
      "Searching for week: 10-03-2023 to 10-10-2023\n",
      "Found 0 articles\n",
      "Searching for week: 10-10-2023 to 10-17-2023\n",
      "Found 100 articles\n",
      "Searching for week: 10-17-2023 to 10-24-2023\n",
      "Found 100 articles\n",
      "Searching for week: 10-24-2023 to 10-31-2023\n",
      "Found 100 articles\n",
      "Searching for week: 10-31-2023 to 11-07-2023\n",
      "Found 100 articles\n",
      "Searching for week: 11-07-2023 to 11-14-2023\n",
      "Found 100 articles\n",
      "Searching for week: 11-14-2023 to 11-21-2023\n",
      "Found 100 articles\n",
      "Searching for week: 11-21-2023 to 11-28-2023\n",
      "Found 99 articles\n",
      "Searching for week: 11-28-2023 to 12-05-2023\n",
      "Found 100 articles\n",
      "Searching for week: 12-05-2023 to 12-12-2023\n",
      "Found 100 articles\n",
      "Searching for week: 12-12-2023 to 12-19-2023\n",
      "Found 100 articles\n",
      "Searching for week: 12-19-2023 to 12-26-2023\n",
      "Found 100 articles\n",
      "Searching for week: 12-26-2023 to 01-02-2024\n",
      "Found 100 articles\n",
      "Searching for week: 01-02-2024 to 01-09-2024\n",
      "Found 100 articles\n",
      "Searching for week: 01-09-2024 to 01-16-2024\n",
      "Found 100 articles\n",
      "Searching for week: 01-16-2024 to 01-23-2024\n",
      "Found 100 articles\n",
      "Searching for week: 01-23-2024 to 01-30-2024\n",
      "Found 100 articles\n",
      "Searching for week: 01-30-2024 to 02-06-2024\n",
      "Found 100 articles\n",
      "Searching for week: 02-06-2024 to 02-13-2024\n",
      "Found 100 articles\n",
      "Searching for week: 02-13-2024 to 02-20-2024\n",
      "Found 100 articles\n",
      "Searching for week: 02-20-2024 to 02-27-2024\n",
      "Found 100 articles\n",
      "Searching for week: 02-27-2024 to 03-05-2024\n",
      "Found 100 articles\n",
      "Searching for week: 03-05-2024 to 03-12-2024\n",
      "Found 97 articles\n",
      "Searching for week: 03-12-2024 to 03-19-2024\n",
      "Found 100 articles\n",
      "Searching for week: 03-19-2024 to 03-26-2024\n",
      "Found 100 articles\n",
      "Searching for week: 03-26-2024 to 04-02-2024\n",
      "Found 100 articles\n",
      "Searching for week: 04-02-2024 to 04-09-2024\n",
      "Found 100 articles\n",
      "Searching for week: 04-09-2024 to 04-16-2024\n",
      "Found 100 articles\n",
      "Searching for week: 04-16-2024 to 04-23-2024\n",
      "Found 100 articles\n",
      "Searching for week: 04-23-2024 to 04-30-2024\n",
      "Found 0 articles\n",
      "Searching for week: 04-30-2024 to 05-07-2024\n",
      "Found 0 articles\n",
      "Searching for week: 05-07-2024 to 05-14-2024\n",
      "Found 0 articles\n",
      "Searching for week: 05-14-2024 to 05-21-2024\n",
      "Found 0 articles\n",
      "Searching for week: 05-21-2024 to 05-28-2024\n",
      "Found 0 articles\n",
      "Searching for week: 05-28-2024 to 06-04-2024\n",
      "Found 0 articles\n",
      "Searching for week: 06-04-2024 to 06-11-2024\n",
      "Found 0 articles\n",
      "Searching for week: 06-11-2024 to 06-18-2024\n",
      "Found 0 articles\n",
      "Searching for week: 06-18-2024 to 06-25-2024\n",
      "Found 0 articles\n",
      "Searching for week: 06-25-2024 to 07-02-2024\n",
      "Found 0 articles\n",
      "Searching for week: 07-02-2024 to 07-09-2024\n",
      "Found 0 articles\n",
      "Scraping complete. Found 9144 articles.\n",
      "Data saved to mergers_acquisitions_articles.csv\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from serpapi.google_search import GoogleSearch\n",
    "import csv\n",
    "\n",
    "API_KEY = \"mykey\"\n",
    "QUERY = \"mergers and acquisitions\"\n",
    "CSV_FILENAME = 'mergers_acquisitions_articles.csv'\n",
    "\n",
    "def get_date_range(end_date):\n",
    "    start_date = end_date - datetime.timedelta(days=7)\n",
    "    return start_date.strftime(\"%m-%d-%Y\"), end_date.strftime(\"%m-%d-%Y\")\n",
    "\n",
    "def search_articles(start_date, end_date):\n",
    "    params = {\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": QUERY,\n",
    "        \"google_domain\": \"google.com\",\n",
    "        \"gl\": \"us\",\n",
    "        \"hl\": \"en\",\n",
    "        \"tbm\": \"nws\",\n",
    "        \"num\": \"100\",\n",
    "        \"start\": \"0\",\n",
    "        \"tbs\": f\"cdr:1,cd_min:{start_date},cd_max:{end_date}\",\n",
    "        \"api_key\": API_KEY\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        search = GoogleSearch(params)\n",
    "        results = search.get_dict()\n",
    "        \n",
    "        articles = []\n",
    "        if 'news_results' in results:\n",
    "            for article in results['news_results']:\n",
    "                articles.append({\n",
    "                    'title': article['title'],\n",
    "                    'link': article['link']\n",
    "                })\n",
    "        return articles\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while searching: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_to_csv(articles, mode='a'):\n",
    "    with open(CSV_FILENAME, mode, newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['title', 'link'])\n",
    "        if mode == 'w':\n",
    "            writer.writeheader()\n",
    "        for article in articles:\n",
    "            writer.writerow(article)\n",
    "\n",
    "# Get the current date and the date 2 years ago\n",
    "end_date = datetime.datetime.now()\n",
    "start_date = end_date - datetime.timedelta(days=365*2)\n",
    "\n",
    "# Initialize CSV file with headers\n",
    "save_to_csv([], mode='w')\n",
    "\n",
    "# Iterate week by week\n",
    "total_articles = 0\n",
    "try:\n",
    "    while start_date < end_date:\n",
    "        week_start, week_end = get_date_range(start_date)\n",
    "        print(f\"Searching for week: {week_start} to {week_end}\")\n",
    "        articles = search_articles(week_start, week_end)\n",
    "        print(f\"Found {len(articles)} articles\")\n",
    "        save_to_csv(articles, mode='a')\n",
    "        total_articles += len(articles)\n",
    "        start_date += datetime.timedelta(days=7)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nScript interrupted by user. Saving collected data...\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    print(f\"Scraping complete. Found {total_articles} articles.\")\n",
    "    print(f\"Data saved to {CSV_FILENAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strangely, I got duplicate articles so I needed to remove them in following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed the input file: mergers_acquisitions_articles.csv\n",
      "Removed 1412 duplicate entries\n",
      "Saved 7732 unique articles to: deduplicated_articles.csv\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def remove_duplicates(input_filename, output_filename):\n",
    "    unique_articles = OrderedDict()\n",
    "    duplicate_count = 0\n",
    "\n",
    "    with open(input_filename, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            link = row['link']\n",
    "            if link not in unique_articles:\n",
    "                unique_articles[link] = row\n",
    "            else:\n",
    "                duplicate_count += 1\n",
    "                \n",
    "    with open(output_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['title', 'link']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in unique_articles.values():\n",
    "            writer.writerow(row)\n",
    "\n",
    "    return len(unique_articles), duplicate_count\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = 'mergers_acquisitions_articles.csv'  \n",
    "    output_filename = 'deduplicated_articles.csv' \n",
    "\n",
    "    unique_count, removed_count = remove_duplicates(input_filename, output_filename)\n",
    "\n",
    "    print(f\"Processed the input file: {input_filename}\")\n",
    "    print(f\"Removed {removed_count} duplicate entries\")\n",
    "    print(f\"Saved {unique_count} unique articles to: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7730\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "csv_file = 'deduplicated_articles.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "df= pd.DataFrame(df)\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the extract_content() function from manual_web_scrap.ipynb to extract the content from the article URLs and we will have our unlabeled data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
